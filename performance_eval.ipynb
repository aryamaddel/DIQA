{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pyiqa\n",
    "\n",
    "# Load LIVE In-the-Wild MOS scores from .mat files\n",
    "print(\"Loading LIVE In-the-Wild dataset...\")\n",
    "mos_data = loadmat(r\"LIVE In the Wild\\Data\\AllMOS_release.mat\")\n",
    "images_data = loadmat(r\"LIVE In the Wild\\Data\\AllImages_release.mat\")\n",
    "\n",
    "# Extract image names and MOS scores\n",
    "if 'AllMOS_release' in mos_data:\n",
    "    mos_scores = mos_data['AllMOS_release'].flatten()\n",
    "else:\n",
    "    mos_scores = list(mos_data.values())[3].flatten()\n",
    "\n",
    "if 'AllImages_release' in images_data:\n",
    "    image_names = [str(img[0]) for img in images_data['AllImages_release'].flatten()]\n",
    "else:\n",
    "    image_names = [str(img[0]) for img in list(images_data.values())[3].flatten()]\n",
    "\n",
    "# LIVE MOS is on 0-100 scale, normalize to 0-5 to match typical IQA output ranges\n",
    "mos_scores_normalized = (mos_scores / 100.0) * 5.0\n",
    "\n",
    "print(f\"Original LIVE MOS range: [{mos_scores.min():.2f}, {mos_scores.max():.2f}]\")\n",
    "print(f\"Normalized LIVE MOS range: [{mos_scores_normalized.min():.2f}, {mos_scores_normalized.max():.2f}]\")\n",
    "\n",
    "# Create DataFrame\n",
    "live_mos_df = pd.DataFrame({\n",
    "    'image_name': image_names,\n",
    "    'MOS_original': mos_scores,\n",
    "    'MOS': mos_scores_normalized\n",
    "})\n",
    "\n",
    "# Get available images\n",
    "live_image_dir = r\"LIVE In the Wild\\Images\"\n",
    "available_images = [\n",
    "    f.name\n",
    "    for f in Path(live_image_dir).iterdir()\n",
    "    if f.suffix.lower() in [\".bmp\", \".jpg\", \".jpeg\"] and f.is_file()\n",
    "]\n",
    "\n",
    "# Filter to only available images\n",
    "live_mos_df = live_mos_df[live_mos_df[\"image_name\"].isin(available_images)]\n",
    "print(f\"\\nEvaluating on {len(live_mos_df)} LIVE In-the-Wild images\")\n",
    "\n",
    "# Initialize IQA metrics\n",
    "iqa_methods = {\n",
    "    'brisque': pyiqa.create_metric('brisque'),\n",
    "    'niqe': pyiqa.create_metric('niqe'),\n",
    "    'piqe': pyiqa.create_metric('piqe'),\n",
    "    'maniqa': pyiqa.create_metric('maniqa'),\n",
    "    'hyperiqa': pyiqa.create_metric('hyperiqa')\n",
    "}\n",
    "\n",
    "# Collect predictions for each method\n",
    "results = {method: [] for method in iqa_methods.keys()}\n",
    "valid_indices = []\n",
    "ground_truth_list = []\n",
    "\n",
    "print(\"\\nComputing IQA scores...\")\n",
    "for idx, row in tqdm(live_mos_df.iterrows(), total=len(live_mos_df)):\n",
    "    image_path = f\"{live_image_dir}/{row['image_name']}\"\n",
    "    \n",
    "    try:\n",
    "        # Compute all IQA scores for this image\n",
    "        all_scores_valid = True\n",
    "        temp_scores = {}\n",
    "        \n",
    "        for method_name, metric in iqa_methods.items():\n",
    "            try:\n",
    "                score = metric(image_path).item()\n",
    "                temp_scores[method_name] = score\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method_name} on {row['image_name']}: {e}\")\n",
    "                all_scores_valid = False\n",
    "                break\n",
    "        \n",
    "        if all_scores_valid:\n",
    "            for method_name, score in temp_scores.items():\n",
    "                results[method_name].append(score)\n",
    "            valid_indices.append(idx)\n",
    "            ground_truth_list.append(row['MOS'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['image_name']}: {e}\")\n",
    "\n",
    "ground_truth = np.array(ground_truth_list)\n",
    "print(f\"\\nSuccessfully processed {len(valid_indices)} images\")\n",
    "\n",
    "# Compute metrics for each method\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LIVE IN-THE-WILD EVALUATION - INDIVIDUAL IQA METHODS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset: {len(valid_indices)} images\")\n",
    "print(f\"Ground Truth MOS range: [{ground_truth.min():.2f}, {ground_truth.max():.2f}]\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for method_name in iqa_methods.keys():\n",
    "    predictions = np.array(results[method_name])\n",
    "    \n",
    "    # Compute correlation metrics (these are scale-invariant)\n",
    "    srocc = spearmanr(predictions, ground_truth)[0]\n",
    "    plcc = pearsonr(predictions, ground_truth)[0]\n",
    "    \n",
    "    # For RMSE/MAE, we need to map predictions to MOS scale\n",
    "    # Use linear regression to map IQA scores to MOS\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(predictions.reshape(-1, 1), ground_truth)\n",
    "    predictions_mapped = reg.predict(predictions.reshape(-1, 1))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth, predictions_mapped))\n",
    "    mae = mean_absolute_error(ground_truth, predictions_mapped)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'Method': method_name.upper(),\n",
    "        'SROCC': srocc,\n",
    "        'PLCC': plcc,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Prediction_Range': f\"[{predictions.min():.2f}, {predictions.max():.2f}]\"\n",
    "    })\n",
    "    \n",
    "    print(f\"{method_name.upper():12s} | SROCC: {srocc:.4f} | PLCC: {plcc:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df.to_csv('live_individual_methods_evaluation.csv', index=False)\n",
    "\n",
    "# Also save detailed predictions\n",
    "detailed_results = pd.DataFrame({\n",
    "    'image_name': live_mos_df.loc[valid_indices, 'image_name'].values,\n",
    "    'ground_truth': ground_truth\n",
    "})\n",
    "\n",
    "for method_name in iqa_methods.keys():\n",
    "    detailed_results[f'{method_name}_score'] = results[method_name]\n",
    "\n",
    "detailed_results.to_csv('live_individual_methods_predictions.csv', index=False)\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation complete!\")\n",
    "print(f\"Summary saved to: live_individual_methods_evaluation.csv\")\n",
    "print(f\"Detailed predictions saved to: live_individual_methods_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
