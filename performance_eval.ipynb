{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9c5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\GitHub\\NR-IQA\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LIVE In-the-Wild dataset...\n",
      "Original LIVE MOS range: [3.42, 92.43]\n",
      "Normalized LIVE MOS range: [0.17, 4.62]\n",
      "\n",
      "Evaluating on 1162 LIVE In-the-Wild images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\GitHub\\NR-IQA\\.venv\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model MANIQA from C:\\Users\\user\\.cache\\torch\\hub\\pyiqa\\ckpt_koniq10k.pt\n",
      "Loading pretrained model HyperNet from C:\\Users\\user\\.cache\\torch\\hub\\pyiqa\\HyperIQA-resnet50-koniq10k-c96c41b1.pth\n",
      "\n",
      "Computing IQA scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [11:59<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 1162 images\n",
      "\n",
      "================================================================================\n",
      "LIVE IN-THE-WILD EVALUATION - INDIVIDUAL IQA METHODS\n",
      "================================================================================\n",
      "Dataset: 1162 images\n",
      "Ground Truth MOS range: [0.17, 4.62]\n",
      "================================================================================\n",
      "\n",
      "BRISQUE      | SROCC: -0.3129 | PLCC: -0.3510 | RMSE: 0.9503 | MAE: 0.7732\n",
      "NIQE         | SROCC: -0.4493 | PLCC: -0.4790 | RMSE: 0.8908 | MAE: 0.7134\n",
      "PIQE         | SROCC: -0.1025 | PLCC: -0.1633 | RMSE: 1.0012 | MAE: 0.8223\n",
      "MANIQA       | SROCC: 0.8323 | PLCC: 0.8392 | RMSE: 0.5519 | MAE: 0.4343\n",
      "HYPERIQA     | SROCC: 0.7495 | PLCC: 0.7734 | RMSE: 0.6434 | MAE: 0.5196\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✓ Evaluation complete!\n",
      "Summary saved to: live_individual_methods_evaluation.csv\n",
      "Detailed predictions saved to: live_individual_methods_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pyiqa\n",
    "\n",
    "# Load LIVE In-the-Wild MOS scores from .mat files\n",
    "print(\"Loading LIVE In-the-Wild dataset...\")\n",
    "mos_data = loadmat(r\"LIVE In the Wild\\Data\\AllMOS_release.mat\")\n",
    "images_data = loadmat(r\"LIVE In the Wild\\Data\\AllImages_release.mat\")\n",
    "\n",
    "# Extract image names and MOS scores\n",
    "if 'AllMOS_release' in mos_data:\n",
    "    mos_scores = mos_data['AllMOS_release'].flatten()\n",
    "else:\n",
    "    mos_scores = list(mos_data.values())[3].flatten()\n",
    "\n",
    "if 'AllImages_release' in images_data:\n",
    "    image_names = [str(img[0]) for img in images_data['AllImages_release'].flatten()]\n",
    "else:\n",
    "    image_names = [str(img[0]) for img in list(images_data.values())[3].flatten()]\n",
    "\n",
    "# LIVE MOS is on 0-100 scale, normalize to 0-5 to match typical IQA output ranges\n",
    "mos_scores_normalized = (mos_scores / 100.0) * 5.0\n",
    "\n",
    "print(f\"Original LIVE MOS range: [{mos_scores.min():.2f}, {mos_scores.max():.2f}]\")\n",
    "print(f\"Normalized LIVE MOS range: [{mos_scores_normalized.min():.2f}, {mos_scores_normalized.max():.2f}]\")\n",
    "\n",
    "# Create DataFrame\n",
    "live_mos_df = pd.DataFrame({\n",
    "    'image_name': image_names,\n",
    "    'MOS_original': mos_scores,\n",
    "    'MOS': mos_scores_normalized\n",
    "})\n",
    "\n",
    "# Get available images\n",
    "live_image_dir = r\"LIVE In the Wild\\Images\"\n",
    "available_images = [\n",
    "    f.name\n",
    "    for f in Path(live_image_dir).iterdir()\n",
    "    if f.suffix.lower() in [\".bmp\", \".jpg\", \".jpeg\"] and f.is_file()\n",
    "]\n",
    "\n",
    "# Filter to only available images\n",
    "live_mos_df = live_mos_df[live_mos_df[\"image_name\"].isin(available_images)]\n",
    "print(f\"\\nEvaluating on {len(live_mos_df)} LIVE In-the-Wild images\")\n",
    "\n",
    "# Initialize IQA metrics\n",
    "iqa_methods = {\n",
    "    'brisque': pyiqa.create_metric('brisque'),\n",
    "    'niqe': pyiqa.create_metric('niqe'),\n",
    "    'piqe': pyiqa.create_metric('piqe'),\n",
    "    'maniqa': pyiqa.create_metric('maniqa'),\n",
    "    'hyperiqa': pyiqa.create_metric('hyperiqa')\n",
    "}\n",
    "\n",
    "# Collect predictions for each method\n",
    "results = {method: [] for method in iqa_methods.keys()}\n",
    "valid_indices = []\n",
    "ground_truth_list = []\n",
    "\n",
    "print(\"\\nComputing IQA scores...\")\n",
    "for idx, row in tqdm(live_mos_df.iterrows(), total=len(live_mos_df)):\n",
    "    image_path = f\"{live_image_dir}/{row['image_name']}\"\n",
    "    \n",
    "    try:\n",
    "        # Compute all IQA scores for this image\n",
    "        all_scores_valid = True\n",
    "        temp_scores = {}\n",
    "        \n",
    "        for method_name, metric in iqa_methods.items():\n",
    "            try:\n",
    "                score = metric(image_path).item()\n",
    "                temp_scores[method_name] = score\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method_name} on {row['image_name']}: {e}\")\n",
    "                all_scores_valid = False\n",
    "                break\n",
    "        \n",
    "        if all_scores_valid:\n",
    "            for method_name, score in temp_scores.items():\n",
    "                results[method_name].append(score)\n",
    "            valid_indices.append(idx)\n",
    "            ground_truth_list.append(row['MOS'])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['image_name']}: {e}\")\n",
    "\n",
    "ground_truth = np.array(ground_truth_list)\n",
    "print(f\"\\nSuccessfully processed {len(valid_indices)} images\")\n",
    "\n",
    "# Compute metrics for each method\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"LIVE IN-THE-WILD EVALUATION - INDIVIDUAL IQA METHODS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset: {len(valid_indices)} images\")\n",
    "print(f\"Ground Truth MOS range: [{ground_truth.min():.2f}, {ground_truth.max():.2f}]\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for method_name in iqa_methods.keys():\n",
    "    predictions = np.array(results[method_name])\n",
    "    \n",
    "    # Compute correlation metrics (these are scale-invariant)\n",
    "    srocc = spearmanr(predictions, ground_truth)[0]\n",
    "    plcc = pearsonr(predictions, ground_truth)[0]\n",
    "    \n",
    "    # For RMSE/MAE, we need to map predictions to MOS scale\n",
    "    # Use linear regression to map IQA scores to MOS\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(predictions.reshape(-1, 1), ground_truth)\n",
    "    predictions_mapped = reg.predict(predictions.reshape(-1, 1))\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(ground_truth, predictions_mapped))\n",
    "    mae = mean_absolute_error(ground_truth, predictions_mapped)\n",
    "    \n",
    "    evaluation_results.append({\n",
    "        'Method': method_name.upper(),\n",
    "        'SROCC': srocc,\n",
    "        'PLCC': plcc,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Prediction_Range': f\"[{predictions.min():.2f}, {predictions.max():.2f}]\"\n",
    "    })\n",
    "    \n",
    "    print(f\"{method_name.upper():12s} | SROCC: {srocc:.4f} | PLCC: {plcc:.4f} | RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "results_df.to_csv('live_individual_methods_evaluation.csv', index=False)\n",
    "\n",
    "# Also save detailed predictions\n",
    "detailed_results = pd.DataFrame({\n",
    "    'image_name': live_mos_df.loc[valid_indices, 'image_name'].values,\n",
    "    'ground_truth': ground_truth\n",
    "})\n",
    "\n",
    "for method_name in iqa_methods.keys():\n",
    "    detailed_results[f'{method_name}_score'] = results[method_name]\n",
    "\n",
    "detailed_results.to_csv('live_individual_methods_predictions.csv', index=False)\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete!\")\n",
    "print(f\"Summary saved to: live_individual_methods_evaluation.csv\")\n",
    "print(f\"Detailed predictions saved to: live_individual_methods_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
